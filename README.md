# PerceptrÃ³n - ClasificaciÃ³n del Dataset Iris

Este proyecto implementa el algoritmo del perceptrÃ³n desde cero para la clasificaciÃ³n binaria del famoso dataset Iris. Incluye mÃºltiples experimentos que demuestran el comportamiento del perceptrÃ³n en diferentes escenarios, desde datos linealmente separables hasta casos no separables.

## ğŸ“‹ Tabla de Contenidos

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Requisitos](#-requisitos)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Uso](#-uso)
- [Experimentos](#-experimentos)
- [Resultados](#-resultados)
- [ContribuciÃ³n](#-contribuciÃ³n)

## ğŸŒŸ CaracterÃ­sticas

- **ImplementaciÃ³n completa del perceptrÃ³n** desde cero
- **MÃºltiples experimentos** con diferentes configuraciones
- **VisualizaciÃ³n interactiva** de regiones de decisiÃ³n
- **AnÃ¡lisis de convergencia** y comportamiento del algoritmo
- **ComparaciÃ³n de casos separables vs no separables**
- **Estudio del impacto de diferentes learning rates**

## ğŸ“¦ Requisitos

### Software necesario:
- Python 3.7 o superior
- pip (gestor de paquetes de Python)

### Dependencias de Python:
```
numpy>=1.19.0
pandas>=1.3.0
matplotlib>=3.3.0
scipy>=1.7.0
urllib3>=1.26.0
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone https://github.com/pedroam-dev/perceptron.git
cd perceptron
```

### 2. Crear un entorno virtual (recomendado)
```bash
# Con venv
python -m venv perceptron_env
source perceptron_env/bin/activate  # En macOS/Linux
# perceptron_env\Scripts\activate   # En Windows

# O con conda
conda create -n perceptron_env python=3.9
conda activate perceptron_env
```

### 3. Instalar dependencias
```bash
pip install numpy pandas matplotlib scipy
```

### 4. Verificar instalaciÃ³n
```bash
python -c "import numpy, pandas, matplotlib, scipy; print('Â¡InstalaciÃ³n exitosa!')"
```

## ğŸ“ Estructura del Proyecto

```
perceptron/
â”‚
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ iris.data                   # Dataset Iris (descarga automÃ¡tica)
â”œâ”€â”€ iris.names.txt              # DescripciÃ³n del dataset
â”‚
â”œâ”€â”€ perceptron.py               # ImplementaciÃ³n base del perceptrÃ³n
â”œâ”€â”€ perceptron_cero.py          # PerceptrÃ³n con inicializaciÃ³n en ceros
â”‚
â”œâ”€â”€ load_and_plot_data.py       # Carga y visualizaciÃ³n de datos
â”œâ”€â”€ training.py                 # Entrenamiento bÃ¡sico
â”œâ”€â”€ training_mod_v1.py          # Entrenamiento con pesos en cero
â”œâ”€â”€ training_mod_v2.py          # Entrenamiento con diferentes Ã©pocas
â”œâ”€â”€ training_mod_v3.py          # AnÃ¡lisis de learning rates
â””â”€â”€ training_no_lineal.py       # Caso no linealmente separable
```

## ğŸ¯ Uso

### Experimento 1: Carga y VisualizaciÃ³n de Datos
```bash
python load_and_plot_data.py
```
**PropÃ³sito:** Cargar el dataset Iris y visualizar la separabilidad de las clases Setosa vs Versicolor usando las caracterÃ­sticas "sepal length" y "petal length".

**Salida esperada:**
- GrÃ¡fico de dispersiÃ³n mostrando las dos clases
- VerificaciÃ³n de separabilidad lineal

---

### Experimento 2: Entrenamiento BÃ¡sico
```bash
python training.py
```
**PropÃ³sito:** Entrenar el perceptrÃ³n bÃ¡sico con diferentes nÃºmeros de Ã©pocas (50, 100, 150).

**Salida esperada:**
- GrÃ¡ficas de convergencia
- VisualizaciÃ³n de regiones de decisiÃ³n
- AnÃ¡lisis de precisiÃ³n

---

### Experimento 3: InicializaciÃ³n con Ceros
```bash
python training_mod_v1.py
```
**PropÃ³sito:** Comparar el comportamiento cuando los pesos se inicializan en cero en lugar de valores aleatorios.

**ParÃ¡metros:**
- Ã‰pocas: 50, 100, 200
- InicializaciÃ³n: wâ‚=0, wâ‚‚=0, bias=0

---

### Experimento 4: AnÃ¡lisis de Learning Rates
```bash
python training_mod_v3.py
```
**PropÃ³sito:** Estudiar cÃ³mo diferentes learning rates [0.2, 0.4, 0.6, 0.8, 1.0] afectan la convergencia.

**MÃ©tricas analizadas:**
- Velocidad de convergencia
- Estabilidad del entrenamiento
- PrecisiÃ³n final
- Tasa de error (1 - accuracy)

---

### Experimento 5: Datos No Separables Linealmente
```bash
python training_no_lineal.py
```
**PropÃ³sito:** Demostrar las limitaciones del perceptrÃ³n con datos no separables usando Versicolor vs Virginica.

**CaracterÃ­sticas utilizadas:**
- Sepal Width vs Petal Width
- Ã‰pocas: 50, 100, 150

**CaracterÃ­sticas especiales:**
- VisualizaciÃ³n de puntos mal clasificados
- LÃ­nea de decisiÃ³n visible
- AnÃ¡lisis de superposiciÃ³n entre clases

## ğŸ§ª Experimentos

### ğŸ“Š Resumen de Experimentos

| Experimento | Archivo | Clases | CaracterÃ­sticas | Separable | Objetivo |
|-------------|---------|--------|----------------|-----------|----------|
| 1 | `load_and_plot_data.py` | Setosa vs Versicolor | Sepal Length, Petal Length | âœ… SÃ­ | VisualizaciÃ³n |
| 2 | `training.py` | Setosa vs Versicolor | Sepal Length, Petal Length | âœ… SÃ­ | Entrenamiento bÃ¡sico |
| 3 | `training_mod_v1.py` | Setosa vs Versicolor | Sepal Length, Petal Length | âœ… SÃ­ | InicializaciÃ³n ceros |
| 4 | `training_mod_v3.py` | Setosa vs Versicolor | Sepal Length, Petal Length | âœ… SÃ­ | Learning rates |
| 5 | `training_no_lineal.py` | Versicolor vs Virginica | Sepal Width, Petal Width | âŒ No | Limitaciones |

### ğŸ›ï¸ ParÃ¡metros Configurables

Para modificar los parÃ¡metros de entrenamiento, edita las siguientes variables en cada archivo:

```python
# Learning rate
eta = 0.1

# NÃºmero de Ã©pocas  
n_iter = 50

# Semilla aleatoria
random_state = 1
```

## ğŸ“ˆ Resultados

### Casos Linealmente Separables
- **Convergencia:** TÃ­picamente en menos de 10 Ã©pocas
- **PrecisiÃ³n:** 100% (clasificaciÃ³n perfecta)
- **Errores finales:** 0 actualizaciones por Ã©poca

### Casos No Separables
- **Convergencia:** No alcanza 0 errores
- **PrecisiÃ³n:** ~70-85% (limitada por superposiciÃ³n)
- **Errores finales:** Persisten actualizaciones por Ã©poca

### Impacto del Learning Rate
- **LR bajo (0.2-0.4):** Convergencia lenta pero estable
- **LR medio (0.6-0.8):** Balance Ã³ptimo
- **LR alto (1.0):** Convergencia rÃ¡pida, posibles oscilaciones

## ğŸ”§ PersonalizaciÃ³n

### Cambiar Dataset
Para usar tus propios datos, modifica la secciÃ³n de carga en cualquier archivo:

```python
# Reemplazar esta secciÃ³n
X = df.iloc[0:100, [0, 2]].values  # tus caracterÃ­sticas
y = np.where(y == 'clase1', 0, 1)  # tus etiquetas
```

### Agregar Nuevas CaracterÃ­sticas
```python
# Ejemplo: usar todas las caracterÃ­sticas
X = df.iloc[0:100, [0, 1, 2, 3]].values
```

### Modificar VisualizaciÃ³n
```python
# Cambiar colores y marcadores
colors = ('green', 'orange', 'purple')
markers = ('x', '+', 'D')
```

## ğŸ› SoluciÃ³n de Problemas

### Error: "No module named 'numpy'"
```bash
pip install numpy pandas matplotlib scipy
```

### Error: "HTTP Error" al descargar datos
El cÃ³digo automÃ¡ticamente usarÃ¡ el archivo local `iris.data` si no puede descargar desde la URL.

### Error: "X is not defined"
AsegÃºrate de ejecutar la secciÃ³n de carga de datos antes del entrenamiento.

### GrÃ¡ficas no se muestran
```bash
# En sistemas sin interfaz grÃ¡fica
pip install matplotlib
# Agregar al cÃ³digo:
import matplotlib
matplotlib.use('Agg')  # Para guardar sin mostrar
```

## ğŸ¤ ContribuciÃ³n

Las contribuciones son bienvenidas. Para contribuir:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“š Referencias

- Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain.
- Dataset Iris: Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems.
- UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris

## ğŸ‘¤ Autor

**Pedro AM** - [@pedroam-dev](https://github.com/pedroam-dev)

---

â­ Si este proyecto te fue Ãºtil, Â¡no olvides darle una estrella!